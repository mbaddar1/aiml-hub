# LLM-Arena

**_The open benchmarking arena for local LLMs._**

---

## ğŸ¯ Purpose

LLM-Arena provides a lightweight and reproducible framework to **benchmark, compare, and analyze local large language models**. Whether you're evaluating Mistral, LLaMA, TinyLLaMA, or Phi, this repo gives you a head-to-head testing ground for instruction-following, summarization, chat, and more â€” all on your machine.

---

## ğŸ“ Repo Structure

```
LLM-Arena/
â”œâ”€â”€ models/           # Loaders and wrappers for various local LLMs
â”œâ”€â”€ prompts/          # Prompt templates for evaluation tasks
â”œâ”€â”€ evaluate.py       # Main script to run evaluations across models
â”œâ”€â”€ metrics.py        # Utility functions for computing metrics (BLEU, ROUGE, etc.)
â”œâ”€â”€ results/          # Store results in JSON/CSV for reproducibility
â”œâ”€â”€ demo.ipynb        # Interactive notebook for exploring outputs
â”œâ”€â”€ README.md         # Project overview and getting started
â””â”€â”€ requirements.txt  # Required Python packages
```

---

## ğŸ›£ Roadmap

1. Create test set group given this paper : https://arxiv.org/html/2412.01020v1

